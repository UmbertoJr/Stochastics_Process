{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Homework 1\"\nauthor: \"Umbertol Junior Mele 1388371\"\ndate: \"30 ottobre 2017\"\noutput: pdf_document\n---\n\n## Ex. 1\n\n***Assume a Dirichlet process (DP) prior, DP(M;G0($\\cdot$)), for distributions G on X.\nShow that for any (measurable) disjoint subsets B1 and B2 of X , Corr(G(B1);G(B2))\nis negative. Is the negative correlation for random probabilities induced by the DP\nprior a restriction? Discuss.***\n\n\\bigskip\n\n\\bigskip\n\n\nA Dirichlet Process is defined in this way:\n\nLet $\\Omega$ be a space and $\\mathbf{A}$ a $\\sigma$-field of subsets, and let be $\\alpha$ be a finite non-null measure on ($\\Omega$,$\\mathbf{A}$). Then a stocastic process $P$ indexed by elements $a$ of $\\mathbf{A}$, is said to be a Dirichlet process on ($\\Omega$,$\\mathbf{A}$) with parameters $\\alpha$ if for any measurable partition ($a_1$,$a_2$, ... ,$a_k$) of $\\Omega$, the random vector ($P(a_1)$, $P(a_2)$, ... , $P(a_k)$) has a Dirichlet distribution with parameter ($\\alpha(a_1)$, $\\alpha(a_2)$, ... , $\\alpha(a_k)$). So $P$ may be considered a random probability measure on ($\\Omega$,$\\mathbf{A}$).\n\nThen taking:$\\Omega = \\mathbf{R}, \\quad \\mathbf{A}=\\mathbf{B}(R)$\n\nLet $G()$ be a function from $R \\to [0,1]$, and  $G_0(a)= \\frac{\\alpha(a)}{\\alpha(\\Omega)}$,(where $M=\\alpha(\\Omega)$ then we can say that:\n\n\n$$G \\sim DP(M,G_0)$$\n\n\nSo, if we made the partition: $B_1, B_2, (B_1 \\cup B_2)^C$, then the $Corr[G(B_1), G(B_2)]$ is:\n\n$$Cov[G(B_1), G(B_2)]= - \\frac{G_0(B_1)\\cdot G_0(B_2)}{(M+1)}$$\n$$Var[G(B_i)]= \\frac{ G_0(B_i)\\bigg(1 - G_0(B_i)\\bigg)}{M+1}$$\n$$Corr[G(B_1),G(B_2)]= \\frac{Cov[G(B_1),G(B_2)]}{\\sqrt{Var[G(B_1)]\\cdot Var[G(B_2)]}}\\leq 0$$\n\n\\bigskip\n\nAnd this properties is peculiar of Dirichlet process. Infact usually we aspect that for a random probability distribution the masses assigned to nearby places increase or decrease together, and this can be a problem that needs to be keeped in mind... because in Dirichlet Process setup if the observations of $B_1$ increase, then the observations of $B_2$, although the sets are close, has to decrease.\n\n\\newpage\n\n## Ex. 2\n\n***Simulation of Dirichlet process prior realizations. Consider a $DP(M;G_0)$ prior over\nthe space of distributions (equivalently c.d.f.s) $G$ on $\\mathbf{R}$, with $G_0 = N(0; 1)$.***\n\n\n\\bigskip\n\n\\bigskip\n\n\na)  To implement Ferguson definition we need to make a partion of $R$ so using intervals like:\n\n$$[-\\infty,x_1], [x_1,x_2],......[x_n,\\infty]$$\nwe are taking disjoint subset from the $\\mathbf{B}(R)$ and since we know that $G \\sim DP(M,G_0)$; then :\n\n$$(g([-\\infty,x_1]), g([x_1,x_2]), ... , g([x_{n}, \\infty]))\\sim Dir(M\\cdot g_0([-\\infty,x_1]),M\\cdot g_0([x_1,x_2]), ... ,M\\cdot g_0([x_{n}, \\infty]))$$\nor better:\n\n$$(G(x_1),G(x_2) - G(x_1), ... ,1 - G(x_n) )\\sim Dir(M\\cdot G_0(x_1),M\\cdot (G_0(x_2) - G_0(x_1)), ... ,M\\cdot (1 - G_0(x_n)))$$\nSo, using Ferduson^1 definition we can sample from a dirichlet distribution using gammas distributions.\n\n$$Y_i \\sim Gamma(1,\\alpha(A_1)) \\quad for \\quad i=1,2,...,k$$\n\n$$S = \\sum_{i=1}^k Y_i \\sim Gamma\\big(1,\\alpha(\\Omega)=M\\big)$$\n\n$$Z_i = \\frac{Y_i}{S}\\quad for \\quad i=1,2,...,k$$\n$$\\mathbf{Z}\\sim Dir(\\alpha(A_1),\\alpha(A_2),...,\\alpha(A_k))$$\n\\bigskip\n\n```{r, fig.height=6}\n\nferguson.def <- function(number.obs, M){\n  sample <- seq(from=-5, to=5, length.out = number.obs)\n  par <- rep(0,number.obs)\n  sim <- rep(0,number.obs)\n  par[1] <- M*pnorm(sample[1])\n  sim[1] <- rgamma(1,par[1])\n  for(i in 2:number.obs){\n    par[i] <- M*(pnorm(sample[i]) - pnorm(sample[i-1]))\n    sim[i] <- rgamma(1,par[i])\n  }\n  tot <- sum(sim)\n  ret <- sim/tot\n  return(ret)\n}\n\nM <- c(5, 20 , 50, 100)\nnumber.obs=100\nsim= 100\na <- list()\nfor(i in 1:4){\n  a[[i]] <- matrix(nrow = sim, ncol = number.obs)\n  for(l in 1:sim)\n  a[[i]][l,] <- ferguson.def(number.obs,M=M[i])\n}\n\n```\n\n\\bigskip\n\nSince from Theorem 3 and Theorem 4 of Ferguson-s paper (1973), we know that:\n\n- if $\\int |Z|d\\alpha < \\infty$, then  $\\int |Z|dP < \\infty$ and...\n\n$$E\\bigg[\\int Z dP\\bigg] = \\int Z dE[P] = \\alpha(\\Omega)^{-1} \\int Z d\\alpha$$\n- if $\\int |Z_1|d\\alpha < \\infty$, $\\int |Z_2|d\\alpha < \\infty$ and $\\int |Z_1Z_2|d\\alpha < \\infty$, then:\n\n$$E\\bigg[ \\int Z_1 dP \\int Z_2 dP \\bigg] = \\frac{\\sigma_{12}}{\\alpha(\\Omega)+1} + \\mu_1 \\mu_2$$\nwhere:\n\n$$\\mu_i = \\alpha(\\Omega)^{-1} \\int Z_i d\\alpha$$\n$$\\sigma_{12} = \\alpha(\\Omega)^{-1}\\cdot \\int Z_1 Z_2 d\\alpha   - \\mu_1 \\mu_2$$\n\n\nThen we can use this results to estimate the mean and the Variance, for prior realization for our case:\n\n$$E\\bigg[\\int t dP(t)\\bigg] = M^{-1}\\int t dMG_0(t) = M^{-1} \\cdot M \\int t dG_0(t) = 0$$\n\n$$E\\bigg[Var P\\bigg] = E[ \\int t^2 dP(t)] - \\bigg(E[\\int t dP(t)]\\bigg)^2 = (\\sigma_0^2 + \\mu_0^2 ) - \\bigg(\\frac{\\sigma_0^2}{M+1} + \\mu_0^2\\bigg) = \\frac{M}{M+1} \\sigma_0^2 = \\frac{M}{M+1}$$\n\n\n\n\n```{r}\nsample <- seq(from=-5, to=5, length.out = number.obs)\npar(mfrow=c(4,4), mar=c(2,2,2,1))\nfor(i in 1:4){\n  el <- apply(a[[i]],1,cumsum )\n  matplot(sample,el, type = 'l', lwd = 0.00001, lty=3,main = paste('M=', M[i]))\n  curve(pnorm, add=T, col='black', lwd=2)\n}\n\nmu.fun <- list(rep(0,sim),rep(0,sim),rep(0,sim),rep(0,sim))\nvar.fun <- list(rep(0,sim),rep(0,sim),rep(0,sim),rep(0,sim))\nfor(m in 1:4){\n  for(i in 1:sim){\n    mu.fun[[m]][i] <- sample%*%a[[m]][i,]\n    var.fun[[m]][i] <- (sample)^2%*%a[[m]][i,] - (sample%*%a[[m]][i,])^2\n  }\n  plot(density(mu.fun[[m]]), main = paste('mean.fun M=', M[m]))\n  segments(x0 = 0, y0 = 0,x1 = 0,y1 = 10)\n}\n\nfor(m in 1:4){\n  plot(density(var.fun[[m]]), main = paste('var.fun M=', M[m]))\n  segments(x0 = (M[m]/(M[m]+1)), y0 = 0,x1 = (M[m]/(M[m]+1)),y1 = 10)\n}\n\n```\n\n\n\\newpage\n\nb)  While using Sethuraman construction:\n\n$$Y_i \\sim G_0$$\n\n$$\\theta_i \\sim Beta(1,M)$$\n\n$$\\pi_1 = \\theta_1$$\n\n$$\\pi_i = \\theta_i \\prod_{j=1}^{i-1}(1 - \\theta_i) \\quad for \\quad i\\geq2$$\n\n$$G(A) = \\sum_1^{\\infty}\\pi_i\\cdot \\delta_{Y_i}(A) \\sim DP(M,G_0)$$\n\n```{r, eval=FALSE}\nsethuraman.cost <- function(number.obs, M){\n  n <- 5000\n  y <- rnorm(n)\n  thet <- rbeta(n,shape1 = 1, shape2 = M)\n  prob <- rep(0,n)\n  prob[1] <- thet[1]\n  for(i in 2:n){\n    prob[i]<- thet[i]*prod(1 - thet[1:i-1])\n  }\n  dat <- sample(y,size= number.obs, prob=prob,replace=T)\n  return(dat)\n}\n\nM <- c(5, 20 , 50, 100)\nnumber.obs=100\nsim= 100\na <- list()\nfor(i in 1:4){\n  a[[i]] <- matrix(nrow = sim, ncol = number.obs)\n  for(l in 1:sim)\n    a[[i]][l,] <- sethuraman.cost(number.obs,M=M[i])\n}\n\n```\n```{r, echo=FALSE,warning=FALSE}\nload(file = 'sethuraman2.RData')\nrequire(randomcoloR, quietly = T)\n```\n\n\\newpage\n\n```{r, warning=FALSE,fig.height=6, fig.width=8}\nsample <- seq(from=-5, to=5, length.out = number.obs)\npar(mfrow=c(4,4), mar=c(2,2,2,1))\nfor(i in 1:4){\n  curve(pnorm, col='black', lwd=2, from = -4, to=4,main = paste('M=', M[i]))\n  for(l in 1:100){\n    plot(ecdf(a[[i]][l,]),verticals=TRUE, do.points=FALSE, \n         lwd = 0.00001, lty=1, add=T,col=randomColor())\n  }\n  curve(pnorm, add=T, col='black', lwd=2)\n}\nfor(i in 1:4){\n  curve(dnorm, col='black', lwd=2, from = -4, to=4,main =paste('pdf M=', M[i]),\n        ylim=c(0,.5))\n  for(l in 1:100){\n    lines(density(a[[i]][l,]), lwd = 0.00001, lty=1\n          ,col=randomColor())\n  }\n  curve(dnorm, add=T, col='black', lwd=2)\n\n}\n\nmu.fun <- list(rep(0,sim),rep(0,sim),rep(0,sim),rep(0,sim))\nvar.fun <- list(rep(0,sim),rep(0,sim),rep(0,sim),rep(0,sim))\nfor(m in 1:4){\n  for(i in 1:sim){\n    mu.fun[[m]][i] <- mean(a[[m]][i,])\n    var.fun[[m]][i] <- var(a[[m]][i,])\n  }\n  plot(density(mu.fun[[m]]), main = paste('mean.fun M=', M[m]))\n  segments(x0 = 0, y0 = 0,x1 = 0,y1 = 10)\n}\n\nfor(m in 1:4){\n  plot(density(var.fun[[m]]), main = paste('var.fun M=', M[m]))\n  segments(x0 = (M[m]/(M[m]+1)), y0 = 0,x1 = (M[m]/(M[m]+1)),y1 = 10)\n}\n\n\n\n\n```\n\n\\newpage\n\nc) Finally, the simulation under a mixture of DPs prior, using a gamma prior for $M$, that is $M \\sim Gamma(3,3)$, that has $E[M] = 1$, and $Var[M]=\\frac{1}{3}$.\n\n$$M \\sim Gamma(3,3)$$\n$$G|M \\sim DP(M,G_0)$$\n\n\nOf course, we can play with the hyperparameters to make stronger believe on the prior distribution.\n\n```{r, eval=FALSE}\nsim = 100\nG <- matrix(nrow = sim,ncol = number.obs)\nfor(t in 1:sim){\n  M <- rgamma(1,3,3)\n  G[t,] <- sethuraman.cost(number.obs,M)\n}\n\n```\n```{r, echo=FALSE}\nload(file = 'MDP.RData')\n```\n\n```{r,echo=FALSE, warning=FALSE}\npar(mfrow=c(1,2), mar=c(3,3,3,3))\ncurve(pnorm, col='black', lwd=2, from = -4, to=4,main = 'M sim from gamma')\nfor(l in 1:100){\n  plot(ecdf(G[l,]),verticals=TRUE, do.points=FALSE,\n       lwd = 0.00001, lty=1,add=T,col=randomColor())\n}\ncurve(pnorm, add=T, col='black', lwd=2)\n\ncurve(dnorm, col='black', lwd=2, from = -4, to=4, ylim=c(0,.5),main = 'pdf with M sim from gamma')\nfor(l in 1:3){\n  lines(density(G[l,]), lwd = 0.00001, lty=1,col=randomColor())\n}\ncurve(dnorm, add=T, col='black', lwd=2)\n\nmu.fun <- rep(0,sim)\nvar.fun <- rep(0,sim)\n\nfor(i in 1:sim){\n  mu.fun[i] <- mean(G[i,])\n  var.fun[i] <- var(G[i,])\n}\nplot(density(mu.fun), main = 'mean distribution')\nsegments(x0 = 0,y0 = 0,x1 = 0,y1 = 10)\nplot(density(var.fun), main = 'variance distribution')\nsegments(x0 = mean(var.fun),y0 = 0,x1 = mean(var.fun),y1 = 10)\n\n```\n\n\nFrom the last plot, we can see that p.d.f. are really strange and this result comes from the fact that sampling from $Gamma(3,3)$, some values of M can be less than 1 and this is the reason of such particular p.d.f sampled from the DP.\nIn this case, furthermore, is hard to compute analitically the mean of the mean functional and variance functional, since M is a random variable and not more a fixed value... so I used a sample estimator.\n\n\\newpage\n\n## Ex.3\n**Posterior inference for one-sample problems using DP priors.**\n\n\\bigskip\n\n1) Simulation for a $N(0,1)$.\n\nSince from theory we know that:\n\n$$G|Y_1,Y_2,...,Y_n \\sim DP\\bigg(M+n, \\frac{1}{M+n}(M\\cdot G_0 + \\sum_{i=1}^n \\delta_{Y_i})\\bigg) $$\n\nthen using Sethuraman contraction we can sampling from the posterior measure $\\frac{1}{M+n}(M\\cdot G_0 + \\sum_{i=1}^n \\delta_{Y_i})$ implementing the _Pòlya urn and the Chinese Restaurant Process_.\n\n\\bigskip\n\n```{r}\nchinese.rest <- function(dati, M, mu, s){\n  pr= M/(M+length(dati))\n  sim <- rep(0,5000)\n  for(i in 1:5000){\n    do <- rbinom(1,1,pr)\n    if(do){\n      sim[i] <- rnorm(1,mu,s)\n    }else{\n      sim[i] <- sample(dati, size=1, replace=T)\n    }\n  }\n  thet <- rbeta(5000,shape1 = 1, shape2 = M+length(dati))\n  prob <- rep(0,5000)\n  prob[1] <- thet[1]\n  for(i in 2:5000){\n    prob[i]<- thet[i]*prod(1 - thet[1:i-1])\n  }\n  dat <- sample(sim, size= 5000, prob=prob,replace=T)\n  return(dat)\n}\n\n```\n\n\n```{r, eval=FALSE}\n# simulation\n\ns.20 <- rnorm(20)\ns.200 <- rnorm(200)\ns.2000 <- rnorm(2000)\n\n\nsim.20 <- matrix(nrow = 100, ncol = 5000)\nsim.200 <- matrix(nrow = 100, ncol = 5000)\nsim.2000 <- matrix(nrow = 100, ncol = 5000)\nfor(s in 1:100){\n  sim.20[s,]<- chinese.rest(s.20,M= 10,0,1)\n  sim.200[s,]<- chinese.rest(s.200,M= 10,0,1)\n  sim.2000[s,]<- chinese.rest(s.2000,M= 10,0,1)\n}\n\n```\n```{r, echo=FALSE, warning=FALSE}\nload(file = 'simulation_normal_m5.RData')\n```\n\n```{r}\ncases <- list(sim.20,sim.200,sim.2000)\ncasi <- c('20 obs','200 obs','2000 obs')\npar(mfrow=c(2,3), mar=c(2,2,1,1))\nfor(m in 1:3){\n  curve(pnorm,  col='black', lwd=2, from = -5,to=5,main = paste(casi[m], 'M=5'))\n  for(i in 1:100){\n    plot(ecdf(cases[[m]][i,]),verticals=TRUE, do.points=FALSE, lwd = 0.00001, lty=1\n         , add=T,col=randomColor())\n  }\n  curve(pnorm,  col='black', lwd=2, from = -5,to=5, add=T)\n  \n}\n\nfor(m in 1:3){\n  curve(dnorm,  col='black', lwd=2, from = -5,to=5,main = paste(casi[m], 'M=5'),\n        ylim=c(0,.5))\n  for(i in 1:100){\n    lines(density(cases[[m]][i,]), lwd = 0.00001, \n          lty=1,col=randomColor())\n  }\n  curve(dnorm,  col='black', lwd=2, from = -5,to=5, add=T)\n}\n\n```\n\\newpage\n\n```{r, echo=FALSE, warning=FALSE}\nload(file = 'simulation_normal_m100.RData')\n```\n\n```{r,echo=FALSE}\ncases <- list(sim.20,sim.200,sim.2000)\ncasi <- c('20 obs','200 obs','2000 obs')\npar(mfrow=c(2,3), mar=c(2,2,1,1))\nfor(m in 1:3){\n  curve(pnorm,  col='black', lwd=2, from = -5,to=5,main = paste(casi[m], 'M=100'))\n  for(i in 1:100){\n    plot(ecdf(cases[[m]][i,]),verticals=TRUE, do.points=FALSE, lwd = 0.00001, lty=1\n         , add=T,col=randomColor())\n  }\n  curve(pnorm,  col='black', lwd=2, from = -5,to=5, add=T)\n  \n}\n\nfor(m in 1:3){\n  curve(dnorm,  col='black', lwd=2, from = -5,to=5,main = paste(casi[m], 'M=100'),\n        ylim=c(0,.5))\n  for(i in 1:100){\n    lines(density(cases[[m]][i,]), lwd = 0.00001, \n          lty=1,col=randomColor())\n  }\n  curve(dnorm,  col='black', lwd=2, from = -5,to=5, add=T)\n}\n\n```\n\n\\newpage\n\n```{r, echo=FALSE, warning=FALSE}\n  load(file = 'simulation_normal_m1000.RData')\n```\n\n```{r,echo=FALSE}\ncases <- list(sim.20,sim.200,sim.2000)\ncasi <- c('20 obs','200 obs','2000 obs')\npar(mfrow=c(2,3), mar=c(2,2,1,1))\nfor(m in 1:3){\n  curve(pnorm,  col='black', lwd=2, from = -5,to=5,main = paste(casi[m], 'M=300'))\n  for(i in 1:100){\n    plot(ecdf(cases[[m]][i,]),verticals=TRUE, do.points=FALSE, lwd = 0.00001, lty=1\n         , add=T,col=randomColor())\n  }\n  curve(pnorm,  col='black', lwd=2, from = -5,to=5, add=T)\n  \n}\n\nfor(m in 1:3){\n  curve(dnorm,  col='black', lwd=2, from = -5,to=5,main = paste(casi[m], 'M=300'),\n        ylim=c(0,.5))\n  for(i in 1:100){\n    lines(density(cases[[m]][i,]), lwd = 0.00001, \n          lty=1,col=randomColor())\n  }\n  curve(dnorm,  col='black', lwd=2, from = -5,to=5, add=T)\n}\n\n```\n\n\n\\newpage\n\n2) Simulation for a mixture of normal distribution.\n\n$$0.5 \\cdot N(2.5,0.5^2) + 0.3 \\cdot N(0.5,0.7^2) + 0.2 \\cdot N(1.5,2^2)$$\n\\newline\n\n```{r, fig.width=10, fig.height=6}\n# simulation of data ------------------------------------------------------\nN <- 10000\ncomponents <- sample(1:3,prob=c(0.5,0.3,0.2),size=N,replace=TRUE)\nmus <- c(2.5,0.5,1.5)\nsds <- sqrt(c(0.5,0.7,2))\n\nsamples <- rnorm(n=N,mean=mus[components],sd=sds[components])\n\npar(mfrow=c(1,2))\nhist(samples, probability = T)\nx <- seq(-20,20,0.01)\n\ntruth <- 0.5*dnorm(x,mean=mus[1],sd=sds[1])+\n  0.3*dnorm(x,mean=mus[2],sd=sds[2])+0.2*dnorm(x,mean=mus[3],sd=sds[3])\nlines(x,truth,col=\"red\",lwd=2)\nlines(density(samples), type = 'l', col='black')\n\ntruth.cdf <- 0.5*pnorm(x,mean=mus[1],sd=sds[1])+\n  0.3*pnorm(x,mean=mus[2],sd=sds[2])+0.2*pnorm(x,mean=mus[3],sd=sds[3])\nplot(ecdf(samples))\nlines(x,truth.cdf,col=\"red\",lwd=2)\n\n\n```\n\n\n\\newpage\n\nSo, using this simulation algorithm, we can create our sample of size:{20,200,2000}\n\n```{r, echo=FALSE, warning=FALSE}\nload(file = 'mixtureofnormalsim.RData')\n  \n```\n\n\nSince from theory we know that:\n\n$$G|Y_1,Y_2,...,Y_n \\sim DP\\bigg(M+n, \\frac{1}{M+n}(M\\cdot G_0 + \\sum_{i=1}^n \\delta_{Y_i})\\bigg) $$\nthen using Sethuraman contraction we can sampling from the posterior mesure $\\frac{1}{M+n}(M\\cdot G_0 + \\sum_{i=1}^n \\delta_{Y_i})$ implementing the **Pòlya urn and the Chinese Restaurant Process**\n\nThe best non-informative values for $m$ and $s^2$ are the sample mean and the sample variance, so I choose to start with this hyperparameters.\n\nWhile the choose of M is low since we know that the true distribution is a mixture of models!\n\nSo for small values of M the model will fit the data aproximately well if we have enough observed data.\n\n\\bigskip\n\n\n```{r, fig.height=10}\nmu <- mean(samples.20)\nse <- sd(samples.20)\n\nrisul <- chinese.rest(samples.20, M = 1, mu = mu, s = se)\n\npar(mfrow=c(3,2))\nplot(ecdf(risul), main = 'posterior c.d.f. sample for M=1')\nlines(x,truth.cdf,col=\"red\",lwd=2)\nhist(risul, probability = T, breaks = 20, main = 'posterior p.d.f. sample for M=1')\nlines(x,truth,col=\"red\",lwd=2)\nlines(density(risul))\n\nload(file = 'simulation_for_m1_20.RData')\n\ncurve(pnorm(x,mu, se),  col='black', lwd=2, from = -5,to=5,main = 'M=1 samp 20')\nfor(i in 1:100){\n  plot(ecdf(samp[i,]),verticals=TRUE, do.points=FALSE, lwd = 0.00001, lty=1\n       , add=T,col=randomColor())\n}\ncurve(pnorm(x,mu, se),  col='black', lwd=2, from = -5,to=5, add=T)\nlines(x,truth.cdf,col=\"red\",lwd=2)\n\ncurve(dnorm(x,mu, se),  col='black', lwd=2, from = -5,to=5,main =  'p.d.f M=5 samp 20',\n      ylim=c(0,.5))\nfor(i in 1:100){\n  lines(density(samp[i,]), lwd = 0.00001, \n        lty=1,col=randomColor())\n}\ncurve(dnorm(x,mu, se),  col='black', lwd=2, from = -5,to=5, add=T)\nlines(x,truth,col=\"red\",lwd=2)\n\nmu.fun <- rep(0,sim)\nvar.fun <- rep(0,sim)\n\nfor(i in 1:sim){\n  mu.fun[i] <- mean(samp[i,])\n  var.fun[i] <- var(samp[i,])\n}\nplot(density(mu.fun), main = 'mean distribution')\nsegments(x0 = mu,y0 = 0,x1 = mu,y1 = 10, col='red')\nplot(density(var.fun), main = 'variance distribution')\nsegments(x0 = se^2,y0 = 0,x1 = se^2,y1 = 10, col = 'red')\n\n\n```\n\n\\newpage\n\n```{r, echo=FALSE, fig.height=10}\nmu <- mean(samples.200)\nse <- sd(samples.200)\n\nrisul <- chinese.rest(samples.200, M = 1, mu = mu, s = se)\n\npar(mfrow=c(3,2))\nplot(ecdf(risul), main='sample 200')\nlines(x,truth.cdf,col=\"red\",lwd=2)\nhist(risul, probability = T, breaks = 20, main='sample 200')\nlines(x,truth,col=\"red\",lwd=2)\nlines(density(risul))\n\nload(file = 'simulation_for_m1_200.RData')\n\ncurve(pnorm(x,mu, se),  col='black', lwd=2, from = -5,to=5,main = 'M=1 samp 200')\nfor(i in 1:100){\n  plot(ecdf(samp[i,]),verticals=TRUE, do.points=FALSE, lwd = 0.00001, lty=1\n       , add=T,col=randomColor())\n}\ncurve(pnorm(x,mu, se),  col='black', lwd=2, from = -5,to=5, add=T)\nlines(x,truth.cdf,col=\"red\",lwd=2)\n\ncurve(dnorm(x,mu, se),  col='black', lwd=2, from = -5,to=5,main =  'p.d.f M=5 samp 200',\n      ylim=c(0,.5))\nfor(i in 1:100){\n  lines(density(samp[i,]), lwd = 0.00001, \n        lty=1,col=randomColor())\n}\ncurve(dnorm(x,mu, se),  col='black', lwd=2, from = -5,to=5, add=T)\nlines(x,truth,col=\"red\",lwd=2)\n\nmu.fun <- rep(0,sim)\nvar.fun <- rep(0,sim)\n\nfor(i in 1:sim){\n  mu.fun[i] <- mean(samp[i,])\n  var.fun[i] <- var(samp[i,])\n}\nplot(density(mu.fun), main = 'mean distribution')\nsegments(x0 = mu,y0 = 0,x1 = mu,y1 = 10, col='red')\nplot(density(var.fun), main = 'variance distribution')\nsegments(x0 = se^2,y0 = 0,x1 = se^2,y1 = 10, col = 'red')\n\n```\n\n\\newpage\n\n```{r, echo=FALSE, fig.height=10}\nmu <- mean(samples.2000)\nse <- sd(samples.2000)\n\nrisul <- chinese.rest(samples.2000, M = 1, mu = mu, s = se)\n\npar(mfrow=c(3,2))\nplot(ecdf(risul), main='sample 2000')\nlines(x,truth.cdf,col=\"red\",lwd=2)\nhist(risul, probability = T, breaks = 20, main='sample 2000')\nlines(x,truth,col=\"red\",lwd=2)\nlines(density(risul))\n\nload(file = 'simulation_for_m1_2000.RData')\n\ncurve(pnorm(x,mu, se),  col='black', lwd=2, from = -5,to=5,main = 'M=1 samp 2000')\nfor(i in 1:100){\n  plot(ecdf(samp[i,]),verticals=TRUE, do.points=FALSE, lwd = 0.00001, lty=1\n       , add=T,col=randomColor())\n}\ncurve(pnorm(x,mu, se),  col='black', lwd=2, from = -5,to=5, add=T)\nlines(x,truth.cdf,col=\"red\",lwd=2)\n\ncurve(dnorm(x,mu, se),  col='black', lwd=2, from = -5,to=5,main =  'p.d.f M=5 samp 2000',\n      ylim=c(0,.5))\nfor(i in 1:100){\n  lines(density(samp[i,]), lwd = 0.00001, \n        lty=1,col=randomColor())\n}\ncurve(dnorm(x,mu, se),  col='black', lwd=2, from = -5,to=5, add=T)\nlines(x,truth,col=\"red\",lwd=2)\n\nmu.fun <- rep(0,sim)\nvar.fun <- rep(0,sim)\n\nfor(i in 1:sim){\n  mu.fun[i] <- mean(samp[i,])\n  var.fun[i] <- var(samp[i,])\n}\nplot(density(mu.fun), main = 'mean distribution')\nsegments(x0 = mu,y0 = 0,x1 = mu,y1 = 10, col='red')\nplot(density(var.fun), main = 'variance distribution')\nsegments(x0 = se^2,y0 = 0,x1 = se^2,y1 = 10, col = 'red')\n\n\n```\n\n\n\n\n\\newpage\n\\center\n\n## Reference\n\n- 1 [*A Bayesian Analysis of some nonparametric problems*](https://projecteuclid.org/download/pdf_1/euclid.aos/1176342360), by Thomas S. Ferguson.\n\n- 2 [*A constructive definition of Dirichlet Priors*](http://www3.stat.sinica.edu.tw/statistica/oldpdf/A4n216.pdf), by Jayaram Sethuraman\n\n- 3 *Bayesian Nonparametrics*, by Nils Lid Hjort, Chris Holmes, Peter Muller and Sthephen G. Walker.\n\n- 4 [*A simple proof of the Stick-Breacking construction of Dirichlet Process*](http://www.columbia.edu/~jwp2128/Teaching/E6892/papers/SimpleProof.pdf), by John Paisley.",
    "created" : 1512474758090.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3781782882",
    "id" : "B2BEAB64",
    "lastKnownWriteTime" : 1510092231,
    "last_content_update" : 1512580032640,
    "path" : "C:/Users/Umbertojunior/Desktop/data science/third Semestr/stochastic process/hw 1/Homework_1/presentazione_hw1.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}